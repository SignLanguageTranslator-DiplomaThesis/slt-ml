import copy

import cv2 as cv
import mediapipe as mp

from constants import constants
from csv_parser.csv_parser import CsvParser
from interface.interface import UserInterface
from interface.annotations import Annotations
from coordinates.coordinates import CoordinateConverter
from model.sign_classifier.classifier import SignClassifier


def main():
    # Camera preparation
    cap = cv.VideoCapture(constants.WEBCAM_DEVICE_INPUT)
    cap.set(cv.CAP_PROP_FRAME_WIDTH, constants.CAPTURE_FRAME_WIDTH)
    cap.set(cv.CAP_PROP_FRAME_HEIGHT, constants.CAPTURE_FRAME_HEIGHT)

    # Model load
    mp_hands = mp.solutions.hands
    hands = mp_hands.Hands(
        static_image_mode=constants.USE_STATIC_IMAGE_MODE,
        max_num_hands=constants.MAX_NUMBER_OF_HANDS,
        min_detection_confidence=constants.MIN_DETECTION_CONFIDENCE,
        min_tracking_confidence=constants.MIN_TRACKING_CONFIDENCE,
    )

    sign_gesture_classifier = SignClassifier()

    # Read labels from sign_classifier_label CSV file
    sign_language_labels = CsvParser.read_sign_labels()

    number = constants.NOT_SELECTED

    while cap.isOpened():
        key = cv.waitKey(10)
        if key == constants.ESC_KEY:
            break
        number, mode = select_mode(key, number, sign_language_labels)

        # Camera capture
        success, image = cap.read()
        if not success:
            print("Ignoring empty camera frame.")
            break

        image = cv.flip(image, 1)  # Mirror display
        edit_image = copy.deepcopy(image)

        # Detection implementation
        image = cv.cvtColor(image, cv.COLOR_BGR2RGB)

        image.flags.writeable = False
        results = hands.process(image)
        image.flags.writeable = True

        if results.multi_hand_landmarks:
            for hand_landmarks, handedness in zip(results.multi_hand_landmarks, results.multi_handedness):
                # Landmark calculation
                coordinate_converter = CoordinateConverter(hand_landmarks)
                coordinate_converter.convert_to_pixel(edit_image)
                coordinate_converter.convert_to_relative_and_normalize()

                # Bounding box calculation
                bounding_box = Annotations.compute_bounding_box(coordinate_converter.pixel_landmarks)

                # Write to the dataset file
                CsvParser.logging_csv(mode, number, coordinate_converter.normalized_landmarks)

                # Hand sign classification
                sign_gesture_index = sign_gesture_classifier.classify(coordinate_converter.normalized_landmarks)

                # Draws the 21 landmark points generated by MediaPipe and the lines connecting them
                edit_image = Annotations.draw_landmarks(edit_image, results.multi_hand_landmarks)

                edit_image = Annotations.draw_info_text(
                    edit_image,
                    bounding_box,
                    handedness,
                    sign_language_labels[sign_gesture_index]
                )

        edit_image = Annotations.draw_info(edit_image, mode)

        # Screen reflection
        cv.imshow('Sign Language Translator', edit_image)

    cap.release()
    cv.destroyAllWindows()


def select_mode(key, number, sign_language_labels):
    user_interface = UserInterface()
    mode = constants.NOT_SELECTED
    if key == constants.CHOOSE_LABEL_MODE:
        user_interface.generate_label_dropdown(sign_language_labels)
        number = user_interface.selected_label_index
    elif key == constants.CREATE_LABEL_MODE:
        user_interface.generate_input_field(sign_language_labels)
    if key in constants.modes:
        mode = key
    return number, mode


if __name__ == '__main__':
    main()
